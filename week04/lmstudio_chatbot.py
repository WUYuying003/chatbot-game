import streamlit as st
from openai import OpenAI

# Point to the local server
client = OpenAI(base_url="http://localhost:1234/v1", api_key="lm-studio")
st.title("ğŸ’¬ Chatbot")
st.caption("ğŸš€ A Streamlit chatbot powered by LMStudio")

if "messages" not in st.session_state:
    st.session_state["messages"] = [{"role": "assistant", "content": "How can I help you?"}]

for msg in st.session_state.messages:
    st.chat_message(msg["role"]).write(msg["content"])

if prompt := st.chat_input():

    st.session_state.messages.append({"role": "user", "content": prompt})
    st.chat_message("user").write(prompt)
    
    try:
        response = client.chat.completions.create(
            model="lmstudio-community/Meta-Llama-3-8B-Instruct-GGUF",
            messages=st.session_state.messages,
            stream=True,
        )

        msg = ""

        def stream_response():
            global msg
            for chunk in response:
                try:
                    part = chunk.choices[0].delta.content
                    if part:
                        msg += part
                        yield part
                except:
                    continue

        
        st.chat_message("assistant").write_stream(stream_response)
        
        st.session_state.messages.append({"role": "assistant", "content": msg})
        
    except Exception as e:
        error_msg = f"âŒ è¿æ¥LM Studioå¤±è´¥: {str(e)}\n\nè¯·ç¡®ä¿:\n1. LM Studioå·²å®‰è£…å¹¶å¯åŠ¨\n2. å·²åŠ è½½æ¨¡å‹\n3. æœ¬åœ°æœåŠ¡å™¨åœ¨ç«¯å£1234è¿è¡Œ\n\næˆ–è€…å°è¯•ä½¿ç”¨OllamaèŠå¤©æœºå™¨äººã€‚"
        st.chat_message("assistant").write(error_msg)
        st.session_state.messages.append({"role": "assistant", "content": error_msg})
